
\section{Problem formulation}
The problem is to compute a 3D-transform $(\MR^*, \vt^*) \in \SEthree$ that optimally aligns two point sequences $\mathcal{P} = \left(\vp_1, ..., \vp_N\right), \mathcal{Q} = \left(\vq_1, ..., \vq_N\right) \in \RthreeByN$ of noisy measurements. We know the correspondences between points ($\vp_i$ and $\vq_i$), but a large fraction (up to 95\%) can be outliers, as it is typical for feature-based matching methods.
\subsection{The Truncated Least Squares problem}
The Truncated Least Squares (TLS) problem is a least squares esitmation problem where residuals are truncadet above a threshold. (It is similar to the Maximum Consensus problem but additionally considers the residual values, obtaining slightly better estimates.)
Outliers are handled by truncating residuals above a threshold $\epsilon^2$, yielding the \textit{Truncated Least Squares} (TLS) problem:

\begin{figure}[!ht]
	\centering
	\begin{adjustbox}{width=1.\linewidth}
		\input{figures/tls_cost_many_term.pgf}
	\end{adjustbox}
	\caption{The non-convex Truncated least-squares (TLS) Funktion with multiple local minima.Shown is the function  $\sum_{i=1}^{N}\min((x - y_i)^2, \epsilon^2)$ for $\epsilon^2=0.1$, and nine $y_i$'s (gray lines).}
	\label{fig:tlscostmulterm}
\end{figure}

Die truncated least squares Funktion ist eine Funktion der Form: 

\begin{equation}
	\begin{aligned}
		\text{TLS}(x) = \sum_{i=1}^{N}\min(r(x, y_i)^2, \epsilon^2)
	\end{aligned}
\end{equation}

where $r(\cdot)$ is a residual function like $\norm{x - \vy_i}$ measuring the deviation of the model given the parameters $x$ and the data points $\vy_i$. This function is shown in Fig. \ref{fig:tlscostmulterm}.

The point cloud registration problem in the TLS formulation is therefore:

\begin{equation}
	\label{eq:pcr-tls}
	\begin{aligned}
		(\MR^*, \vt^*) =  \argmin_{(\MR, \vt) \in \SEthree}  \sum_{i=1}^{N} \min \left(\normsq{\MR \vp_i  - \vq_i  + \vt}, \epsilon^2 \right)
	\end{aligned}
\end{equation}

Among the many ways to make least squares regression robust to outliers \cite[Ch. 3]{elements-of-stats-learning-book}, the TLS formulation has recently gained populariy \cite{Yang20tro-teaser, 9785843, NIPS2017_9f53d83e, doi:10.1080/10618600.2017.1390471, NIPS2010_01882513} since it has been shown to be very robust even to high outlier rates of 90\% \cite{Yang20tro-teaser}.
This problem is a non-convex combinatorial problem that is difficult to solve (requiring non-convex binary constraints \cite{5459398}).

The parameter $\epsilon$ is best understood as the known measurement accuracy of each point. It can be estimated by collecting samples, i.e. in a data-driven manner \cite[p.8]{Chin2017TheMC}.


\section{Wheighted Least Squares convex relaxation}

In this section we propose a novel convex relaxation for the Truncated Least Squares problem that can be computed in linear time.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/wls_relax_idea_with_relax}
	\caption{The idea of the WLS convex relaxation: We compute the maximum $r_{max}$ of each convex term (orange) over the given interval (blue) and then reweight it such that it remains below $\epsilon^2$ over this interval and is thus not truncated over the entire interval (green).}
	\label{fig:wlsrelaxideawithrelax}
\end{figure}

First, consider the truncated least squares objective for a general residual term $r(\cdot)$ that depends on the model (i.e rotation or translation): 

\begin{equation}
	\label{eq:tls-scalar-for-wls}
	\begin{aligned}
		\text{TLS}(\mathbf{x}) = \sum_{i=1}^{N} \min \left(r(\mathbf{x}, \mathbf{x}_i)^2, \epsilon^2 \right)\\
	\end{aligned}
\end{equation}

where $\mathbf{x}_i$ are data points and $r(\cdot)^2$ is some convex residual function such as the squared Euclidean distance $||\mathbf{x} - \mathbf{x}_i||_2^2$. In the following, we will refer to the i-th residual term $r(\mathbf{x}, \mathbf{x_i})^2$ simply as $r_i$.
The idea of the WLS-relaxation is the observation that this function is only non-convex because of the truncation -- the residuals are convex (parabolas). Also, sums of convex functions are convex (this is the reason why ordinary least squares is a convex problem), so we can focus on making each term convex separately.

For this, we first find the minimum and maximum values of each residual $r_i$ is over the given interval, denoted as $r_{min}^i$ and $r_{max}^i$ respectively. The key geometric idea of our relaxation is to reweight these residuals such that after reweighting, their maximum becomes $\epsilon^2$ and thus they are not affected by truncation (Fig. \ref{fig:wlsrelaxideawithrelax}).

However, some additional special case handling is required to ensure that this relaxation becomes tight when computing over (small) intervals where the TLS objective is already convex. This is crucial to ensure that BnB converges in a finite number of iterations. First, the reweighting should only be applied to residuals where $r_{min} < \epsilon^2$, i.e., residuals that are not already outliers over the entire interval. Similarly, residuals where $r_{max} < \epsilon^2$ are inliers over the entire interval and are therefore already convex and do not need to be reweighted.

After considering these special cases, we can show that $\text{WLS}_{r}$ is a convex relaxation of \ref{eq:tls-scalar-for-wls} over a given interval:
\begin{equation}
	\label{eq:tls-wls-relaxation1}
	\begin{aligned}	
		\text{WLS}_{r} &= 
		\sum_{i=1}^{N} (1 - o_i) \left(w_i r_i  + (1 - w_i) r_{min}^i \right) + \sum_{i=1}^{N} o_i\epsilon^2\\
	\end{aligned}
\end{equation}

with the weights $w_i$ defined as:
\begin{equation}
	\begin{aligned}	
		w_i  = 
		\begin{cases}
			\frac{\epsilon^2 - r_{min}^i}{r_{max}^i - r_{min}^i} &  r_{max}^i > \epsilon^2\\
			1 & \, \text{otherwise}
		\end{cases}
	\end{aligned}
\end{equation}

and the binary variables $o_i$ deciding whether the $i$-th residual is an outlier or not:
\begin{equation}
	\begin{aligned}	
		o_i &= \begin{cases}
			1 &  r_{min}^i > \epsilon^2\\
			0 & \, \text{otherwise}
		\end{cases}
	\end{aligned}
\end{equation}

For a proof of this WLS-relaxation, see \ref{proof:tls-convex-wls-relaxation}.


\subsection{Solving the equality-constrained case}


If the inequality-constraint is not satisfied after the first unconstrained solve, we need to solve the problem with equality-constraint, that is the following optimization problem: 

\begin{equation}
	\label{eq:tls-rwls-eq}
	\begin{aligned}
		\argmin_{\Delta \mathbf{R} \in \SOthree}  \quad &\sum_{i=1}^{N} w_i ||\mathbf{R}_c^\transposed \mathbf{a}_i - \Delta\mathbf{R} \mathbf{b}_i ||_2^2 \\
		\text{subject to} \quad &d_{\angle}(\mathbf{I}, \Delta\mathbf{R}) = n_r
	\end{aligned}
\end{equation}

We can show that this optimization problem is equivalent to the following problem that finds the optimal axis of rotation $\mathbf{n}^*$:

\begin{equation}
	\begin{aligned}
		\label{eq:eq-constrained-prob-ev}
		\mathbf{n}^* = \argmin_{\mathbf{n} \in \Rthree} \quad &-\left(\mathbf{n}^\transposed \mathbf{A} \mathbf{n} + 2 \mathbf{g}^\transposed \mathbf{n} \right)\\
		\text{subject to} \quad &|| \mathbf{n}|| = 1\\
	\end{aligned}
\end{equation}

with the data matrices $\mathbf{A}$ and $\mathbf{g}$ that depend on the points $\mathbf{a}_i$ and $\mathbf{b}_i$ and the angle constraint $n_r$:

\begin{equation}
	\label{eq:davenport-data-matrices}
	\begin{aligned}
		\mathbf{B} &= \sum_{i=1}^N w_i \mathbf{R}_c^\transposed \mathbf{a}_i\mathbf{b}_i^\transposed \in \Rone^{3 \times 3}, \quad
		\mathbf{z} = \sum_{i=1}^N w_i [\mathbf{a}_i]_{\times}\mathbf{b}_i \in \Rone^{3}, \quad \mathbf{C} = \mathbf{B} + \mathbf{B}^\transposed\\
		\mathbf{A} &= \sin^2 \left(\frac{n_r}{2} \right) \mathbf{C}, \quad \mathbf{g} =  \sin \left(\frac{n_r}{2} \right) \cos \left(\frac{n_r}{2} \right) \mathbf{z}
	\end{aligned}
\end{equation}

This derivation is based on the Daveport's Quaternion-method \cite{davenportsQ} \cite{8594296}, see \ref{proof:wls-relax-eq-constrained-rotation} for a proof of the equivalence of both optimization problems.

To solve the optimization problem \ref{eq:eq-constrained-prob-ev}, we perform overall two steps: We first need to compute the eigenvalue decomposition  $\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^\transposed$ and then need to find largest root of the following rational function $C(\gamma)$ (characteristic polynomial): 

\begin{equation}
	\label{eq:characterisitc-poly}
	\begin{aligned}
		C(\gamma) = \sum_{i=1}^3 \left(\frac{a_i}{\lambda_i - \gamma}\right)^2 - 1
	\end{aligned}
\end{equation}

with
\begin{equation}
	\begin{aligned}
		\mathbf{a} = (a_1, a_2, a_3)^\transposed = \mathbf{Q}^\transposed \mathbf{g} \in \Rthree, \quad \mathbf{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_3)
	\end{aligned}
\end{equation}

\begin{algorithm}[!ht]
	\caption{Solve constrained eigenvalue problem}
	\label{alg:solve-constrained-ev}
	\begin{algorithmic}[1]
		\Require{Data matrices $\mathbf{A} \in \Rone^{3\times3}, \mathbf{g} \in \Rthree$}
		\Ensure Solution $\mathbf{n}^*$ to the optimization problem Eq. \ref{eq:eq-constrained-prob-ev}
		\hypertarget{ref:min-constrained-ev}{
			\Function{\code{solve\_constrained\_ev}}{$\mathbf{A}, \mathbf{g}$}}
		\State $\code{kEps} \leftarrow 2^{-48}$ \algorithmiccomment{Epsilon-value for double precision}
		\State $\mathbf{Q}, \mathbf{\Lambda} \leftarrow \code{eig(} \mathbf{A} \code{)}$ \algorithmiccomment{Compute eigen-decomposition}
		\State $k = \argmax\{\lambda_1, \lambda_2, \lambda_3\}$
		\State $\mathbf{a} \leftarrow \mathbf{Q}^\transposed \mathbf{g}$
		\State $l = \lambda_k + |a_k| - \code{kEps}, r = ||\mathbf{a}|| + \lambda_k + \code{kEps}$ \algorithmiccomment{Bracket root}
		\State $\hat{\gamma} \leftarrow \code{bisect} (C, [l, r] \code{)}$ \algorithmiccomment{Find root of Eq.\ref{eq:characterisitc-poly} with bisection}
		\State $\mathbf{f} \leftarrow \left(\frac{a_1}{\lambda_1 - \hat{\gamma}}, \frac{a_2}{\lambda_2 - \hat{\gamma}}, \frac{a_3}{\lambda_3 - \hat{\gamma}} \right)$
		\State $\mathbf{n}^* \leftarrow \mathbf{Q}\mathbf{f}$ 
		\State \Return $\mathbf{n}^*$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

This algorithm is shown in Alg \ref{alg:solve-constrained-ev}, see \ref{sec:constrained-ev-problem-derivation} for a derivation with explanations.
The eigen-decomposition can exploit the fact that the matrix is real and symmetric (but not positive semi-definite) for which faster algorithms are available (i.e. Eigen's \code{Eigen::SelfAdjointEigenSolver}). Note also that the vector $\mathbf{f}$ in line 8 is equal to $(\mathbf{\Lambda} - \hat{\gamma} \mathbf{I})^{-1} \mathbf{a}$. No actual matrix-inversion is required however since the matrix is diagonal and also with floating-point arithmetic it is numerically more stable/accurate to compute $a/b$ instead of $\frac{1}{b} a$. 


\subsection{Overall active-set solver}

\begin{algorithm}[!ht]
	\caption{Active-set method for minimizing the WLS-relaxation over rotations (\ref{eq:tls-rwls})}
	\label{alg:min-wls-relax-rot}
	\begin{algorithmic}[1]
		\Require{Point cloud matrices $\mathbf{A}, \mathbf{B}$, node interval $(\mathbf{R_c}, n_r)$, computed WLS-relaxation weights vector $\mathbf{w}$}
		\Ensure Solution to the optimization problem \ref{eq:tls-rwls}
		\hypertarget{ref:min-wls-relax-rot2}{
			\Function{\code{Minimize\_WLS\_Relax\_Rotation}}{$\mathbf{A}, \mathbf{B}, \mathbf{R_c}, n_r, \mathbf{w}$}}
		\State $\mathbf{B} \leftarrow \sum_{i=1}^N w_i \mathbf{R}_c^\transposed \mathbf{a}_i\mathbf{b}_i^\transposed $ \algorithmiccomment{Compute cross-correlation matrix}
		\State $\bar{\Delta \mathbf{R}} \leftarrow \code{svd\_rot(} \mathbf{B} \code{)}$  \algorithmiccomment{Solve unconstrained with SVD-algorithm}
		\If{$d_{\angle}(\mathbf{I}, \Delta\mathbf{R}) \leq n_r$} \algorithmiccomment{Check if constraint is already satisfied}
		\State \Return $\mathbf{R_c} \bar{\Delta \mathbf{R}}$
		\EndIf
		\State $\mathbf{A}, \mathbf{g} \leftarrow \code{get\_davenport\_matrices(} \mathbf{B}, n_r \code{)}$ \algorithmiccomment{Get  data-matrices (Eq. \ref{eq:davenport-data-matrices})}
		\State $\mathbf{n}^* \leftarrow \code{solve\_constrained\_ev(} \mathbf{A}, \mathbf{g} \code{)}$ \algorithmiccomment{Solve for
			rotation axis (Eq. \ref{eq:eq-constrained-prob-ev})}
		\State $\bar{\Delta \mathbf{R}} \leftarrow \code{from\_axis\_angle(}\mathbf{n}^*, n_r \code{)}$ \algorithmiccomment{ Construct optimal rotation}
		\State \Return $\mathbf{R_c} \bar{\Delta \mathbf{R}}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}


Our active-set solver for finding the global minimum of the WLS-relaxation over rotations is shown in Alg. \ref{alg:min-wls-relax-rot}. It is very efficient since the only computationally intensive operation is the computation of the $3 \times 3$ cross-correlation matrix $\mathbf{B}$ (line 13) that has linear runtime in the number of points. All the operations that follow are based on this $3 \times 3$ matrix and therefore have a constant runtime (i.e. independent of the number of points) and are in practice very fast. For example, the eigen-decomposition (line 3) takes only around $10 \mu s$ while the computation of the cross-correlation matrix requires aroundd $500 \mu s$ for 10k points. This is also because the cross-correlation matrix is re-used between the SVD-algorithm (line 3) and the algorithm for the equality-constraint case (line 7). The function \code{from\_axis\_angle} converts a given axis and angle to a rotation matrix using Rodrigues' formula.

This solver is novel to our knowledge -- the problem of solving the Wahba-problem with inequality-constrained angle has not been proposed in the literature before.

The solver for the equality-constraint case ($\code{solve\_constrained\_ev}$) is implemented from the derivation first done in \cite{10.1007/978-3-642-75536-1_57} and inspired by the \textit{Trust-Region Subproblem} (TRS) that arises in trust-region (local) optimization algorithms \cite{Adachi2017SolvingTT} such as Levenberg-Marquardt. For such TRS-problems, an active-set method has been proposed also in \cite{Rontsis2022} where it was generalized additionally for a minimum-norm constraints.

