
\section{Problem formulation}
The problem is to compute a 3D-transform $(\MR^*, \vt^*) \in \SEthree$ that optimally aligns two point sequences $\mathcal{P} = \left(\vp_1, ..., \vp_N\right), \mathcal{Q} = \left(\vq_1, ..., \vq_N\right) \in \RthreeByN$ of noisy measurements. We know the correspondences between points ($\vp_i$ and $\vq_i$), but a large fraction (up to 95\%) can be outliers, as it is typical for feature-based matching methods.
\subsection{The Truncated Least Squares problem}
\begin{figure}[!ht]
	\centering
	\begin{adjustbox}{width=1.\linewidth}
		\input{figures/tls_cost_many_term.pgf}
	\end{adjustbox}
	\caption{The non-convex Truncated least-squares (TLS) function with multiple local minima.Shown is the function  $\sum_{i=1}^{N}\min((x - y_i)^2, \epsilon^2)$ for $\epsilon^2=0.1$, and nine $y_i$'s (gray lines).}
	\label{fig:tlscostmulterm}
\end{figure}

The Truncated Least Squares (TLS) problem is a least squares estimation problem where residuals are truncated above a threshold. (It is similar to the Maximum Consensus problem but additionally considers the residual values, obtaining slightly better estimates.)
Outliers are handled by truncating residuals above a threshold $\epsilon^2$, the objective function is therefore:


\begin{equation}
	\begin{aligned}
		\text{TLS}(x) = \sum_{i=1}^{N}\min(r(x, y_i)^2, \epsilon^2)
	\end{aligned}
\end{equation}

where $r(\cdot)$ is a residual function like $\norm{x - \vy_i}$ measuring the deviation of the model given the parameters $x$ and the data points $\vy_i$. This function is shown in Fig. \ref{fig:tlscostmulterm}.

The point cloud registration problem in the TLS formulation is therefore:

\begin{equation}
	\label{eq:pcr-tls}
	\begin{aligned}
		(\MR^*, \vt^*) =  \argmin_{(\MR, \vt) \in \SEthree}  \sum_{i=1}^{N} \min \left(\normsq{\MR \vp_i  - \vq_i  + \vt}, \epsilon^2 \right)
	\end{aligned}
\end{equation}

Among the many ways to make least squares regression robust to outliers \cite[Ch. 3]{elements-of-stats-learning-book}, the TLS formulation has recently gained populariy \cite{Yang20tro-teaser, 9785843, NIPS2017_9f53d83e, doi:10.1080/10618600.2017.1390471, NIPS2010_01882513} since it has been shown to be very robust even to high outlier rates of 90\% \cite{Yang20tro-teaser}.
This problem is a non-convex combinatorial problem that is difficult to solve (requiring non-convex binary constraints \cite{5459398}).

The parameter $\epsilon$ is best understood as the known measurement accuracy of each point. It can be estimated by collecting samples, i.e. in a data-driven manner \cite[p.8]{Chin2017TheMC}.


\section{Wheighted Least Squares convex relaxation}

In this section we propose a novel convex relaxation for the Truncated Least Squares problem that can be computed in linear time.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/wls_relax_idea_with_relax}
	\caption{The idea of the WLS convex relaxation: We compute the maximum $r_{max}$ of each convex term (orange) over the given interval (blue) and then reweight it such that it remains below $\epsilon^2$ over this interval and is thus not truncated over the entire interval (green).}
	\label{fig:wlsrelaxideawithrelax}
\end{figure}

First, consider the truncated least squares objective for a general residual term $r(\cdot)$ that depends on the model (i.e rotation or translation): 

\begin{equation}
	\label{eq:tls-scalar-for-wls}
	\begin{aligned}
		\text{TLS}(\mathbf{x}) = \sum_{i=1}^{N} \min \left(r(\mathbf{x}, \mathbf{x}_i)^2, \epsilon^2 \right)\\
	\end{aligned}
\end{equation}

where $\mathbf{x}_i$ are data points and $r(\cdot)^2$ is some convex residual function such as the squared Euclidean distance $||\mathbf{x} - \mathbf{x}_i||_2^2$. In the following, we will refer to the i-th residual term $r(\mathbf{x}, \mathbf{x_i})^2$ simply as $r_i$.
The idea of the WLS-relaxation is the observation that this function is only non-convex because of the truncation -- the residuals are convex (parabolas). Also, sums of convex functions are convex (this is the reason why ordinary least squares is a convex problem), so we can focus on making each term convex separately.

For this, we first find the minimum and maximum values of each residual $r_i$ is over the given interval, denoted as $r_{min}^i$ and $r_{max}^i$ respectively. The key geometric idea of our relaxation is to reweight these residuals such that after reweighting, their maximum becomes $\epsilon^2$ and thus they are not affected by truncation (Fig. \ref{fig:wlsrelaxideawithrelax}).

However, some additional special case handling is required to ensure that this relaxation becomes tight when computing over (small) intervals where the TLS objective is already convex. This is crucial to ensure that BnB converges in a finite number of iterations. First, the reweighting should only be applied to residuals where $r_{min} < \epsilon^2$, i.e., residuals that are not already outliers over the entire interval. Similarly, residuals where $r_{max} < \epsilon^2$ are inliers over the entire interval and are therefore already convex and do not need to be reweighted.

After considering these special cases, we can show that $\text{WLS}_{r}$ is a convex relaxation of \ref{eq:tls-scalar-for-wls} over a given interval:
\begin{equation}
	\label{eq:tls-wls-relaxation1}
	\begin{aligned}	
		\text{WLS}_{r} &= 
		\sum_{i=1}^{N} (1 - o_i) \left(w_i r_i  + (1 - w_i) r_{min}^i \right) + \sum_{i=1}^{N} o_i\epsilon^2\\
	\end{aligned}
\end{equation}

with the weights $w_i$ defined as:
\begin{equation}
	\begin{aligned}	
		w_i  = 
		\begin{cases}
			\frac{\epsilon^2 - r_{min}^i}{r_{max}^i - r_{min}^i} &  r_{max}^i > \epsilon^2\\
			1 & \, \text{otherwise}
		\end{cases}
	\end{aligned}
\end{equation}

and the binary variables $o_i$ deciding whether the $i$-th residual is an outlier or not:
\begin{equation}
	\begin{aligned}	
		o_i &= \begin{cases}
			1 &  r_{min}^i > \epsilon^2\\
			0 & \, \text{otherwise}
		\end{cases}
	\end{aligned}
\end{equation}

For a proof of this WLS-relaxation, see \ref{proof:tls-convex-wls-relaxation}.

\subsection{Interval analysis of residuals}

In the following we show how $r_{min}$ and $r_{max}$ can easily be computed analytically for different models (i.e. translation and rotation), given spherical intervals, i.e. balls.

The minimum $r_{min}^i$ and maximum $r_{max}^i$ of the residual $r_i =||\mathbf{a}_i - \mathbf{b}_i + \mathbf{t}||_2^2$ (translation) over a ball with center $\mathbf{c}$ and radius $n_r$ are: (see \ref{proof:tls-relax-minmax-r} for a proof):

\begin{equation}
	\begin{aligned}	
		r_{min}^i &= \max(0, ||\mathbf{d}_i||_2 - n_r)^2\\
		r_{max}^i &= (||\mathbf{d}_i||_2 + n_r)^2
	\end{aligned}
\end{equation}

with $\mathbf{d}_i$ defined as: 

\begin{equation}
	\begin{aligned}	
		\mathbf{d}_i = \mathbf{a}_i - \mathbf{b}_i + \mathbf{c}
	\end{aligned}
\end{equation}

Similarly, for the rotation residual $||\mathbf{a}_i - \mathbf{R}\mathbf{b}_i ||_2^2$ over a over a ball with center $\mathbf{R}_c$ and (geodesic) radius $n_r$ they are (see \ref{proof:wls-interval-analysis-rotation} for a proof): 

\begin{equation}
	\label{eq:residual-minmax-rotation}
	\begin{aligned}
		r_{min}^i = ||\mathbf{a}_i||^2 + ||\mathbf{b}_i||^2 - 2 ||\mathbf{a}_i|| \, ||\mathbf{b}_i|| \cos (\max(\theta - n_r, 0))\\
		r_{max}^i = ||\mathbf{a}_i||^2 + ||\mathbf{b}_i||^2 - 2 ||\mathbf{a}_i|| \, ||\mathbf{b}_i|| \cos (\min(\theta + n_r, \pi))
	\end{aligned}
\end{equation}

where $\theta$ is the angle between $\mathbf{R}_c^\transposed \,\mathbf{a}_i$ and $\mathbf{b}_i$:

\begin{equation}
	\begin{aligned}
		\theta = \arccos \left( \frac{(\mathbf{R}_c^\transposed \, \mathbf{a}_i )^\transposed \mathbf{b}_i}{|| \mathbf{a}_i || \, || \mathbf{b}_i ||} \right)
	\end{aligned}
\end{equation}

Computing these intervals requires constant time and is in practice very fast when using trigonometric identities to elimitade the the need for evaluating the cosine function.


\section{Minimizing the WLS convex relaxation}
After having computed the WLS relaxation (i.e. the weights), we need to minimize it to obtain a lower bound on the TLS objective. 
Since the relaxation is a simple weighted least squares function, minimizing it is rather easy. The only thing we need to consider is that we seek the minimum that is inside the Branch and Bound node. We therefore need to introduce an inequality constraint into the problem.
This constrained problem can be solved can still be solved very efficiently by utilizing custom active-set solvers that are only slightly more expensive than solving the least-squares problem.

\subsection{Minimizing the WLS-relaxation over translations}

To minimize the WLS-relaxation over the translation $\mathbf{t}$ for obtaining the lower bound over the ball with center $\mathbf{c}$ and radius $n_r$, we need to solve the following optimization problem:
\begin{equation}
	\label{eq:tls-twls-constrained}
	\begin{aligned}
		\argmin_{\mathbf{x} \in \Rone^3} \quad &\sum_{i=1}^{N} w_i ||\mathbf{d}_i - \mathbf{x}||_2^2 \\
		\text{subject to} \quad  &||\mathbf{x}||_2 \leq n_r
	\end{aligned}
\end{equation}

where 
\begin{equation}
	\begin{aligned}
		\mathbf{x} = \mathbf{c} - \mathbf{t}  \iff  \mathbf{t} = \mathbf{c} - \mathbf{x}\\
		\mathbf{d}_i = \mathbf{a}_i - \mathbf{b}_i + \mathbf{c}
	\end{aligned}
\end{equation}

The constraint $||\mathbf{x}||_2 \leq n_r$ ensures that the found $\mathbf{x}$ is inside the BnB node. The other sums of the convex relaxation \ref{eq:tls-wls-relaxation1} disappeared because they are constant (i.e. they do not depend on $\mathbf{x}$). This problem is a convex QCQP and therefore solvable by any standard QCQP solver. However, we show that the solution can be obtained even more easily in closed form, which in practice results in a much faster algorithm.

We first form the Lagrangian to account for the constraint (which is squared): 

\begin{equation}
	\begin{aligned}	
		L(\mathbf{x}, \lambda) =  \sum_{i=1}^{N} w_i ||\mathbf{d}_i - \mathbf{x}||_2^2 + \lambda (
		\mathbf{x}^\transposed \mathbf{x} - n_r^2)\\
	\end{aligned}
\end{equation}

then, we set the first partial derivatives to zero: 
\begin{equation}
	\begin{aligned}	
		\frac{\partial L}{\partial \mathbf{x}} &= \frac{\partial }{\partial \mathbf{x}} \left( \sum_{i=1}^{N} w_i \left(\mathbf{d}_i - \mathbf{x}\right)^\transposed \left(\mathbf{d}_i - \mathbf{x}\right) + \lambda (
		\mathbf{x}^\transposed \mathbf{x} - n_r^2) \right)\\
		&= - 2\sum_{i=1}^{N} w_i \left(\mathbf{d}_i - \mathbf{x}\right) + 2 \lambda \mathbf{x}\\
		&= \mathbf{0}\\
	\end{aligned}
\end{equation}


By rearranging, we can obtain therefore the solution in closed form:
\begin{equation}
	\label{eq:trans-qcqp-lagrangian}
	\begin{aligned}	
		-2  &\sum_{i=1}^{N}  w_i \left(\mathbf{d}_i - \mathbf{x}\right) + 2 \lambda \mathbf{x} = \mathbf{0}\\
		\iff \lambda \mathbf{x} &= \sum_{i=1}^{N} w_i \left(\mathbf{d}_i - \mathbf{x}\right)\\
		\lambda \mathbf{x} &= \sum_{i=1}^{N}  w_i \mathbf{d}_i  - \sum_{i=1}^{N}  w_i \mathbf{x}\\
		\lambda \mathbf{x} &= \sum_{i=1}^{N}  w_i \mathbf{d}_i  - \mathbf{x} \sum_{i=1}^{N}  w_i\\
		\left( \lambda + \sum_{i=1}^{N}  w_i \right) \mathbf{x} &= \sum_{i=1}^{N} w_i \mathbf{d}_i \\
		\mathbf{x} &= \frac{1}{\lambda + \sum_{i=1}^{N}  w_i} \sum_{i=1}^{N} w_i \mathbf{d}_i 
	\end{aligned}
\end{equation}

Now the Karush-Kuhn-Tucker (KKT) conditions require $\lambda \geq 0$ and $\lambda (
\mathbf{x}^\transposed \mathbf{x} - n_r^2) = 0$ in addition to satisfying the constraint, i.e. $|| \mathbf{x}||_2 \leq n_r$. It follows that in practice we obtain the optimum by first computing $\mathbf{x}$ with Eq. \ref{eq:trans-qcqp-lagrangian}, assuming $\lambda = 0$. Then we check whether the inequality constraint is satisfied, i.e. whether $|| \mathbf{x}||_2 \leq n_r$ holds, and if not, we simply normalise $\mathbf{x}$ so that $|| \mathbf{x}||_2 = n_r$, yielding the optimal solution. Intuitively, from Eq. \ref{eq:trans-qcqp-lagrangian} we already see that the direction of the vector does not change regardless of the optimal Lagrange multiplier $\lambda$ since it is only a scalar factor.  

This optimization problem can be understood as a special case of the Trust Region Problem for which strong duality holds \cite[Ch. 5.2, p.229]{Boyd_Vandenberghe_2004} as well as a regularized linear regression problem.



\subsection{Minimizing the WLS-relaxation over rotations}

To minimize the convex relaxation over rotations and obtain a lower bound for given BnB-node that is a ball with center $\mathbf{R_c}$ and (geodesic) radius $n_r$, we need to solve the following optimization problem:

\begin{equation}
	\label{eq:tls-rwls}
	\begin{aligned}
		\argmin_{\Delta \mathbf{R} \in \SOthree}  \quad &\sum_{i=1}^{N} w_i ||\mathbf{R}_c^\transposed \mathbf{a}_i - \Delta\mathbf{R} \mathbf{b}_i ||_2^2 \\
		\text{subject to} \quad &d_{\angle}(\mathbf{I}, \Delta\mathbf{R}) \leq n_r
	\end{aligned}
\end{equation}

Where $\Delta\mathbf{R}$ is the rotation relative to the node's center: 

\begin{equation}
	\begin{aligned}
		\Delta\mathbf{R} = \mathbf{R_c}^\transposed\mathbf{R} \iff \mathbf{R_c}\Delta\mathbf{R} = \mathbf{R}
	\end{aligned}
\end{equation}

The unconstrained version of this problem is known as the \textit{Wahba-problem} for which a simple SVD-algorithm exists, known as the Kabsch-Umeyama algorithm \cite{Kabsch-1978-Point-set-alignment} \cite{Least-squares-estimation-point-sets-Umeyama-1991} \cite{Lawrence2019APA}. But this problem is modified and has in addition an inequality-constraint. In the following, we develop a custom solver that solves this problem very efficiently. 

The first idea is to just try to solve the unconstrained version and check whether it satisfies the inequality constraint (i.e. the solution is inside the ball). 
We can do that, but unfortunately the solution does not always satisfy the inequality constraint. We gain however an important insight: If the solution satisfies the constraint and lies therefore in the spherical region, we have already found the optimum of the constrained problem. If however the optimum of the unconstrained problem is outside of the region, the optimal solution to the constrained problem has to lie on the border of the region. It then has to satisfy the constraint as an equality, i.e. $ d_{\angle}(\mathbf{I}, \Delta\mathbf{R})  = n_r$ \cite[Ch.15 p.424-427]{Numerical-Optimization-Nocedal-Wright}.
This two-stage method for solving inequality-constrained nonlinear problems is known as the \textit{active-set} method \cite[Ch. 16.5, p.467]{Numerical-Optimization-Nocedal-Wright}, an inequality constraint is set to be active if the solution satisfies it as an equality. 

Using this active-set approach, we can effectively solve this inequality-constrained optimization problem, since we already have the SVD-algorithm for the unconstrained case. And for equality-constrained case, we make the observation that by Euler's theorem that says that every 3D-rotation can be separated into a n axis and angle, we only have to optimize over the axis when the angle is equality-constrained and therefore fixed.

As an algorithm outline, we therefore first solve the unconstrained problem with the SVD-algorithm and check whether it satisfies the inequality-constraint. If not, we solve for the equality-constrained problem where we seek for the rotation that has the rotation angle $n_r$, that is more difficult case.


\subsection{Solving the equality-constrained case}

If the inequality-constraint is not satisfied after the first unconstrained solve, we need to solve the problem with equality-constraint, that is the following optimization problem: 

\begin{equation}
	\label{eq:tls-rwls-eq}
	\begin{aligned}
		\argmin_{\Delta \mathbf{R} \in \SOthree}  \quad &\sum_{i=1}^{N} w_i ||\mathbf{R}_c^\transposed \mathbf{a}_i - \Delta\mathbf{R} \mathbf{b}_i ||_2^2 \\
		\text{subject to} \quad &d_{\angle}(\mathbf{I}, \Delta\mathbf{R}) = n_r
	\end{aligned}
\end{equation}

We can show that this optimization problem is equivalent to the following problem that finds the optimal axis of rotation $\mathbf{n}^*$:

\begin{equation}
	\begin{aligned}
		\label{eq:eq-constrained-prob-ev}
		\mathbf{n}^* = \argmin_{\mathbf{n} \in \Rthree} \quad &-\left(\mathbf{n}^\transposed \mathbf{A} \mathbf{n} + 2 \mathbf{g}^\transposed \mathbf{n} \right)\\
		\text{subject to} \quad &|| \mathbf{n}|| = 1\\
	\end{aligned}
\end{equation}

with the data matrices $\mathbf{A}$ and $\mathbf{g}$ that depend on the points $\mathbf{a}_i$ and $\mathbf{b}_i$ and the angle constraint $n_r$:

\begin{equation}
	\label{eq:davenport-data-matrices}
	\begin{aligned}
		\mathbf{B} &= \sum_{i=1}^N w_i \mathbf{R}_c^\transposed \mathbf{a}_i\mathbf{b}_i^\transposed \in \Rone^{3 \times 3}, \quad
		\mathbf{z} = \sum_{i=1}^N w_i [\mathbf{a}_i]_{\times}\mathbf{b}_i \in \Rone^{3}, \quad \mathbf{C} = \mathbf{B} + \mathbf{B}^\transposed\\
		\mathbf{A} &= \sin^2 \left(\frac{n_r}{2} \right) \mathbf{C}, \quad \mathbf{g} =  \sin \left(\frac{n_r}{2} \right) \cos \left(\frac{n_r}{2} \right) \mathbf{z}
	\end{aligned}
\end{equation}

This derivation is based on the Daveport's Quaternion-method \cite{davenportsQ} \cite{8594296}, see \ref{proof:wls-relax-eq-constrained-rotation} for a proof of the equivalence of both optimization problems.

To solve the optimization problem \ref{eq:eq-constrained-prob-ev}, we perform overall two steps: We first need to compute the eigenvalue decomposition  $\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^\transposed$ and then need to find largest root of the following rational function $C(\gamma)$ (characteristic polynomial): 

\begin{equation}
	\label{eq:characterisitc-poly}
	\begin{aligned}
		C(\gamma) = \sum_{i=1}^3 \left(\frac{a_i}{\lambda_i - \gamma}\right)^2 - 1
	\end{aligned}
\end{equation}

with
\begin{equation}
	\begin{aligned}
		\mathbf{a} = (a_1, a_2, a_3)^\transposed = \mathbf{Q}^\transposed \mathbf{g} \in \Rthree, \quad \mathbf{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_3)
	\end{aligned}
\end{equation}

\begin{algorithm}[!ht]
	\caption{Solve constrained eigenvalue problem}
	\label{alg:solve-constrained-ev}
	\begin{algorithmic}[1]
		\Require{Data matrices $\mathbf{A} \in \Rone^{3\times3}, \mathbf{g} \in \Rthree$}
		\Ensure Solution $\mathbf{n}^*$ to the optimization problem Eq. \ref{eq:eq-constrained-prob-ev}
		\hypertarget{ref:min-constrained-ev}{
			\Function{\code{solve\_constrained\_ev}}{$\mathbf{A}, \mathbf{g}$}}
		\State $\code{kEps} \leftarrow 2^{-48}$ \algorithmiccomment{Epsilon-value for double precision}
		\State $\mathbf{Q}, \mathbf{\Lambda} \leftarrow \code{eig(} \mathbf{A} \code{)}$ \algorithmiccomment{Compute eigen-decomposition}
		\State $k = \argmax\{\lambda_1, \lambda_2, \lambda_3\}$
		\State $\mathbf{a} \leftarrow \mathbf{Q}^\transposed \mathbf{g}$
		\State $l = \lambda_k + |a_k| - \code{kEps}, r = ||\mathbf{a}|| + \lambda_k + \code{kEps}$ \algorithmiccomment{Bracket root}
		\State $\hat{\gamma} \leftarrow \code{bisect} (C, [l, r] \code{)}$ \algorithmiccomment{Find root of Eq.\ref{eq:characterisitc-poly} with bisection}
		\State $\mathbf{f} \leftarrow \left(\frac{a_1}{\lambda_1 - \hat{\gamma}}, \frac{a_2}{\lambda_2 - \hat{\gamma}}, \frac{a_3}{\lambda_3 - \hat{\gamma}} \right)$
		\State $\mathbf{n}^* \leftarrow \mathbf{Q}\mathbf{f}$ 
		\State \Return $\mathbf{n}^*$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

This algorithm is shown in Alg \ref{alg:solve-constrained-ev}, see \ref{sec:constrained-ev-problem-derivation} for a derivation with explanations.
The eigen-decomposition can exploit the fact that the matrix is real and symmetric (but not positive semi-definite) for which faster algorithms are available (i.e. Eigen's \code{Eigen::SelfAdjointEigenSolver}). Note also that the vector $\mathbf{f}$ in line 8 is equal to $(\mathbf{\Lambda} - \hat{\gamma} \mathbf{I})^{-1} \mathbf{a}$. No actual matrix-inversion is required however since the matrix is diagonal and also with floating-point arithmetic it is numerically more stable/accurate to compute $a/b$ instead of $\frac{1}{b} a$. 


\subsection{Overall active-set solver}

\begin{algorithm}[!ht]
	\caption{Active-set method for minimizing the WLS-relaxation over rotations (\ref{eq:tls-rwls})}
	\label{alg:min-wls-relax-rot}
	\begin{algorithmic}[1]
		\Require{Point cloud matrices $\mathbf{A}, \mathbf{B}$, node interval $(\mathbf{R_c}, n_r)$, computed WLS-relaxation weights vector $\mathbf{w}$}
		\Ensure Solution to the optimization problem \ref{eq:tls-rwls}
		\hypertarget{ref:min-wls-relax-rot2}{
			\Function{\code{Minimize\_WLS\_Relax\_Rotation}}{$\mathbf{A}, \mathbf{B}, \mathbf{R_c}, n_r, \mathbf{w}$}}
		\State $\mathbf{B} \leftarrow \sum_{i=1}^N w_i \mathbf{R}_c^\transposed \mathbf{a}_i\mathbf{b}_i^\transposed $ \algorithmiccomment{Compute cross-correlation matrix}
		\State $\bar{\Delta \mathbf{R}} \leftarrow \code{svd\_rot(} \mathbf{B} \code{)}$  \algorithmiccomment{Solve unconstrained with SVD-algorithm}
		\If{$d_{\angle}(\mathbf{I}, \Delta\mathbf{R}) \leq n_r$} \algorithmiccomment{Check if constraint is already satisfied}
		\State \Return $\mathbf{R_c} \bar{\Delta \mathbf{R}}$
		\EndIf
		\State $\mathbf{A}, \mathbf{g} \leftarrow \code{get\_davenport\_matrices(} \mathbf{B}, n_r \code{)}$ \algorithmiccomment{Get  data-matrices (Eq. \ref{eq:davenport-data-matrices})}
		\State $\mathbf{n}^* \leftarrow \code{solve\_constrained\_ev(} \mathbf{A}, \mathbf{g} \code{)}$ \algorithmiccomment{Solve for
			rotation axis (Eq. \ref{eq:eq-constrained-prob-ev})}
		\State $\bar{\Delta \mathbf{R}} \leftarrow \code{from\_axis\_angle(}\mathbf{n}^*, n_r \code{)}$ \algorithmiccomment{ Construct optimal rotation}
		\State \Return $\mathbf{R_c} \bar{\Delta \mathbf{R}}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}


Our active-set solver for finding the global minimum of the WLS-relaxation over rotations is shown in Alg. \ref{alg:min-wls-relax-rot}. It is very efficient since the only computationally intensive operation is the computation of the $3 \times 3$ cross-correlation matrix $\mathbf{B}$ (line 13) that has linear runtime in the number of points. All the operations that follow are based on this $3 \times 3$ matrix and therefore have a constant runtime (i.e. independent of the number of points) and are in practice very fast. For example, the eigen-decomposition (line 3) takes only around $10 \mu s$ while the computation of the cross-correlation matrix requires aroundd $500 \mu s$ for 10k points. This is also because the cross-correlation matrix is re-used between the SVD-algorithm (line 3) and the algorithm for the equality-constraint case (line 7). The function \code{from\_axis\_angle} converts a given axis and angle to a rotation matrix using Rodrigues' formula.

This solver is novel to our knowledge -- the problem of solving the Wahba-problem with inequality-constrained angle has not been proposed in the literature before.

The solver for the equality-constraint case ($\code{solve\_constrained\_ev}$) is implemented from the derivation first done in \cite{10.1007/978-3-642-75536-1_57} and inspired by the \textit{Trust-Region Subproblem} (TRS) that arises in trust-region (local) optimization algorithms \cite{Adachi2017SolvingTT} such as Levenberg-Marquardt. For such TRS-problems, an active-set method has been proposed also in \cite{Rontsis2022} where it was generalized additionally for a minimum-norm constraints.

